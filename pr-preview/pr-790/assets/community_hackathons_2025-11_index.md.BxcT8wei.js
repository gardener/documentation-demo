import{_ as t,c as a,o,a2 as r}from"./chunks/framework.Gi3Q71dA.js";const i="/pr-preview/pr-790/assets/group-picture.h2x9xnzu.jpeg",n="/pr-preview/pr-790/assets/concept.BEw7gsQu.png",s="/pr-preview/pr-790/assets/prow-without-cache.BJfn4J2v.png",l="/pr-preview/pr-790/assets/prow-with-cache.0G-Xy1ap.png",f=JSON.parse('{"title":"November 2025","description":"","frontmatter":{"github_repo":"https://github.com/gardener/documentation","github_subdir":"website/community/hackathons","outline":2,"params":{"github_branch":"master"},"path_base_for_github_subdir":{"from":"content/community/hackathons/2025-11.md","to":"2025-11.md"},"title":"November 2025","weight":-202511,"prev":false,"next":false},"headers":[],"relativePath":"community/hackathons/2025-11/index.md","filePath":"community/hackathons/2025-11.md","lastUpdated":null}'),d={name:"community/hackathons/2025-11/index.md"};function c(h,e,u,p,m,g){return o(),a("div",null,e[0]||(e[0]=[r('<h1 id="hack-the-garden-11-2025-wrap-up" tabindex="-1">Hack The Garden 11/2025 Wrap Up <a class="header-anchor" href="#hack-the-garden-11-2025-wrap-up" aria-label="Permalink to &quot;Hack The Garden 11/2025 Wrap Up&quot;">‚Äã</a></h1><ul><li>üóìÔ∏è <strong>Date:</strong> 24.11.2025 ‚Äì 28.11.2025</li><li>üìç <strong>Location:</strong> <a href="https://www.schlosshof-info.de/" target="_blank" rel="noreferrer">Schlosshof Freizeitheim, Schelklingen</a></li><li>üë§ <strong>Organizer:</strong> <a href="https://www.x-cellent.com/" target="_blank" rel="noreferrer">x-cellent</a></li><li>üìò <strong>Topics:</strong> <a href="https://hackmd.io/c-SxOnnDTE-XQbrXnrPprw" target="_blank" rel="noreferrer">https://hackmd.io/c-SxOnnDTE-XQbrXnrPprw</a></li><li>üé§ <strong>Review Meeting Summary:</strong> <a href="https://gardener.cloud/community/review-meetings/2025-reviews/#_2025-12-03-hack-the-garden-wrap-up" target="_blank" rel="noreferrer">https://gardener.cloud/community/review-meetings/2025-reviews/#_2025-12-03-hack-the-garden-wrap-up</a></li></ul><p><img src="'+i+'" alt="Group picture"></p><h2 id="üê∂-use-self-hosted-shoot-cluster-for-single-node-e2e-tests" tabindex="-1">üê∂ Use Self-Hosted Shoot Cluster For Single-Node E2E Tests <a class="header-anchor" href="#üê∂-use-self-hosted-shoot-cluster-for-single-node-e2e-tests" aria-label="Permalink to &quot;üê∂ Use Self-Hosted Shoot Cluster For Single-Node E2E Tests&quot;">‚Äã</a></h2><h3 id="problem-statement" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>Our goal is to increase confidence and robustly test self-hosted shoot clusters by integrating them into our existing end-to-end (E2E) testing framework. We plan to run these E2E tests within a single-node, self-hosted <code>Shoot</code> cluster. We will leverage the ability to create such clusters using <code>gardenadm</code>.</p><h3 id="motivation-benefits" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>Running E2E tests in a self-hosted shoot cluster allows us to <em>&quot;eat our own dog food&quot;</em>. This provides a real-world test for the self-hosted shoot functionality. This will ultimately increase confidence in the stability and reliability of self-hosted shoot clusters.</p><h3 id="achievements" tabindex="-1">Achievements <a class="header-anchor" href="#achievements" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><p>We made significant progress across several core areas to enable this setup:</p><ul><li><em>Setup/bootstrap:</em> Successfully ran <code>gardenadm init</code> in a Docker container. Tracked as <code>gind</code> (<strong>G</strong>ardener <strong>in</strong> <strong>D</strong>ocker, inspired by <a href="https://kind.sigs.k8s.io/" target="_blank" rel="noreferrer">KinD</a>).</li><li><em>etcd management:</em> Made druid-managed etcd optional. This allows the system to continue with a bootstrap etcd if no backup is configured in the <code>Shoot</code> manifest.</li><li><em>Networking/DNS:</em><ul><li>Addressed <code>NetworkPolicies</code> for CoreDNS.</li><li>Fixed registry hostnames by always running registries as containers via <code>docker compose</code> and exposing them on dedicated hostnames.</li><li>Cleaned up the provider-local <code>Service</code> controller in the traditional <code>kind</code>-based setup.</li><li>Fixed DNS records in <code>kind</code> CoreDNS <code>Corefile</code> to use hard-coded IPs.</li></ul></li><li><em>Controller conflict management:</em> Ensured that multiple controllers (e.g., gardener-resource-manager, etcd-druid, vpa) do not conflict.</li><li><em>Host network:</em> Ensured gardener-resource-manager and extensions remain in the host network.</li></ul><h3 id="next-steps" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>The following tasks are planned to complete the goal and address ongoing considerations:</p><ul><li><em>Extension management:</em><ul><li>We need to complete the logic for extension management to prevent extensions from being deployed twice if the self-hosted shoot is also a <code>Seed</code>.</li><li>This involves the <code>gardenlet</code> labeling the <code>Seed</code> object if it runs in a self-hosted <code>Shoot</code>.</li><li>The gardener-controller-manager (GCM) must then adapt its <code>ControllerInstallation</code> creation logic accordingly.</li></ul></li><li><em>E2E integration:</em> We need to adapt existing E2E tests to create the self-hosted cluster and run tests within it.</li><li><em>Tooling:</em> We will implement <code>gind</code> (Gardener/gardenadm in Docker) as a <code>kind</code> alternative using <code>gardenadm</code>.</li><li><em>Controller isolation (future):</em> We plan to move gardener-resource-manager and etcd-druid of the self-hosted <code>Shoot</code> to the garden namespace to avoid potential conflicts/interferences when the self-hosted <code>Shoot</code> is also the garden runtime or a <code>Seed</code> cluster.</li></ul><h3 id="code-pull-requests" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>Introduce <code>--use-bootstrap-etcd</code> flag for <code>gardenadm init</code>: <a href="https://github.com/gardener/gardener/pull/13542" target="_blank" rel="noreferrer">gardener/gardener#13542</a></li><li>Always run registries as containers via <code>docker compose</code> and expose on dedicated hostnames: <a href="https://github.com/gardener/gardener/pull/13551" target="_blank" rel="noreferrer">gardener/gardener#13551</a></li><li>Cleanup <code>provider-local Service</code> controller in traditional <code>kind</code>-based setup: <a href="https://github.com/gardener/gardener/pull/13549" target="_blank" rel="noreferrer">gardener/gardener#13549</a></li><li>Implement <code>gind</code> (<strong>G</strong>ardener/<code>gardenadm</code> in Docker): <a href="https://github.com/timebertt/gardener/tree/gind" target="_blank" rel="noreferrer">timebertt/gardener:gind</a></li></ul><h2 id="ü´Ü-enrich-shoot-logs-with-istio-access-logs" tabindex="-1">ü´Ü Enrich Shoot Logs with Istio Access Logs <a class="header-anchor" href="#ü´Ü-enrich-shoot-logs-with-istio-access-logs" aria-label="Permalink to &quot;ü´Ü Enrich Shoot Logs with Istio Access Logs&quot;">‚Äã</a></h2><h3 id="problem-statement-1" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-1" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>The Istio ingress gateway generates valuable access logs that show all requests passing through it, especially in conjunction with Layer 7 (L7) load balancing. Currently, these logs are only accessible to <code>Seed</code> operators, limiting visibility for <code>Shoot</code> cluster users. We need to move these logs to the corresponding shoot log stream to enhance cluster debugging and operational visibility for <code>Shoot</code> owners, particularly in cases where access control is restricted (e.g., by the ACL extension).</p><h3 id="motivation-benefits-1" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-1" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><ul><li>Improved debugging and operational visibility for <code>Shoot</code> cluster owners by giving them direct access to ingress traffic logs.</li><li>Enhanced compliance and security by making critical access information available within the shoot&#39;s boundary, which is necessary when strict access controls are in place.</li><li>This project serves as a pilot for enriching <code>Shoot</code> logs with logs from other relevant <code>Seed</code> components in the future.</li></ul><h3 id="achievements-1" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-1" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><ul><li>We successfully implemented the required changes to enrich <code>Shoot</code> logs with Istio access logs.</li><li>This functionality was integrated into the main Gardener repository following an initial attempt in the logging extension repository.</li></ul><h3 id="next-steps-1" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-1" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><ul><li>We should consider extending this approach to include access logs or other relevant logs from other <code>Seed</code> components that impact <code>Shoot</code> cluster operations.</li><li>The feature needs validation across different <code>Shoot</code> configurations to ensure stability and reliable delivery of the access logs to the <code>Shoot</code> log stream.</li></ul><h3 id="code-pull-requests-1" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-1" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>Initial attempt (closed): <a href="https://github.com/gardener/logging/pull/398" target="_blank" rel="noreferrer">gardener/logging#398</a></li><li>Final implementation: <a href="https://github.com/gardener/gardener/pull/13548" target="_blank" rel="noreferrer">gardener/gardener#13548</a></li></ul><h2 id="ü™£-allow-relocating-backup-buckets" tabindex="-1">ü™£ Allow Relocating Backup Buckets <a class="header-anchor" href="#ü™£-allow-relocating-backup-buckets" aria-label="Permalink to &quot;ü™£ Allow Relocating Backup Buckets&quot;">‚Äã</a></h2><h3 id="problem-statement-2" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-2" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>Currently, there is no straightforward way to relocate ETCD backups for shoots, such as changing the backup provider or region. This can only be achieved using a <code>Shoot</code> migration to another <code>Seed</code> that has a different backup provider configured. Such a migration can be difficult because it requires downtime for <code>Shoot</code> owners, as all <code>Shoot</code>s managed by a <code>Seed</code> need to be migrated. For operator setups, it is not possible to change the backup location either because the respective fields in the <code>Garden</code> resource are also immutable.</p><h3 id="motivation-benefits-2" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-2" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>We want to be able to relocate backups more easily and without any downtime for the <code>Shoot</code> owners. During our migration to <code>gardener-operator</code>, we created the Garden cluster in a completely new cloud provider project, but existing <code>Seed</code>s still point to the old project&#39;s backup bucket. We would like to migrate these buckets to the new project so we can clean up the old one.</p><h3 id="achievements-2" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-2" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><p>We created a small prototype to allow mutating the fields for relocating the backup. The proposed changes allow mutating the fields such that a redeployment of the ETCD <code>StatefulSet</code> gets triggered during the next <code>Shoot</code> reconciliation. This <code>StatefulSet</code> includes the new backup configuration. As soon as the new ETCD pod starts, it continues to run with the data found on the PVC and then directly creates a full snapshot in the new backup bucket after startup. This behavior needs no modification in the backup-restore sidecar. The prototype implementation also allows changing the bucket name used by the <code>Seed</code>s, which is currently strictly derived from the <code>Seed</code> UID.</p><h3 id="next-steps-2" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-2" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>We need to finalize the implementation and open a pull request. It might be worth opening another issue to discuss modifying the backup-restore sidecar to allow it to &quot;claim&quot; the bucket it acts on.</p><h3 id="code-pull-requests-2" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-2" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>Issue: <a href="https://github.com/gardener/gardener/issues/13579" target="_blank" rel="noreferrer">gardener/gardener#13579</a></li><li>Prototype implementation: <a href="https://github.com/metal-stack/gardener/tree/relocate-backups" target="_blank" rel="noreferrer">metal-stack/gardener:relocate-backups</a></li></ul><h2 id="ü™û-pull-gardener-node-agent-from-registry-mirror" tabindex="-1">ü™û Pull <code>gardener-node-agent</code> From Registry Mirror <a class="header-anchor" href="#ü™û-pull-gardener-node-agent-from-registry-mirror" aria-label="Permalink to &quot;ü™û Pull `gardener-node-agent` From Registry Mirror&quot;">‚Äã</a></h2><h3 id="problem-statement-3" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-3" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>It is currently not possible to pull the <code>gardener-node-agent</code> through a registry mirror configured by the <code>OperatingSystemConfig</code> (OSC) extension. This is because the <code>gardener-node-agent</code> is the component responsible for configuring <code>containerd</code> with the registry mirrors, creating a chicken-and-egg problem during the initial <code>Node</code> provisioning.</p><h3 id="motivation-benefits-3" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-3" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>We want to enable the <code>gardener-node-agent</code> to be pulled directly from a configured registry mirror, which simplifies the initial <code>Node</code> setup and ensures that all image pulls respect the configured mirroring policies from the very start. We previously used a workaround where the registry mirror was added as <code>systemd</code> files into the userdata via a webhook, but integrating this logic directly into the <a href="https://github.com/gardener/gardener-extension-registry-cache" target="_blank" rel="noreferrer"><code>registry-cache</code> extension</a> is cleaner and more robust.</p><h3 id="achievements-3" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-3" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><ul><li>Introduced an option in the extension to specify a <code>Mirror</code> as <code>provisionRelevant</code>. Marking a mirror as <code>provisionRelevant</code> causes the mirror configuration to be included as files in the <code>provision</code> <code>OperatingSystemConfig</code>. This ensures the mirror configuration is available before the <code>gardener-node-agent</code> is running, allowing the agent to be pulled via the mirror.</li><li>After the <code>gardener-node-agent</code> is running, it will take over and maintain the <code>containerd</code> config, updating it if needed.</li><li>Added the possibility to set a CA bundle for a <code>Mirror</code>.</li></ul><h3 id="next-steps-3" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-3" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>The pull request needs to be finalized, documentation updated, and then merged and released.</p><h3 id="code-pull-requests-3" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-3" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>Pull request: <a href="https://github.com/gardener/gardener-extension-registry-cache/pull/495" target="_blank" rel="noreferrer">gardener/gardener-extension-registry-cache#495</a></li></ul><h2 id="üóΩ-evaluate-talos-as-node-operating-system" tabindex="-1">üóΩ Evaluate <a href="https://www.talos.dev/" target="_blank" rel="noreferrer">Talos</a> As Node Operating System <a class="header-anchor" href="#üóΩ-evaluate-talos-as-node-operating-system" aria-label="Permalink to &quot;üóΩ Evaluate [Talos](https://www.talos.dev/) As Node Operating System&quot;">‚Äã</a></h2><h3 id="problem-statement-4" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-4" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>We evaluated the technical feasibility of integrating <a href="https://www.talos.dev/" target="_blank" rel="noreferrer">Talos OS</a>, a modern, minimal, and immutable operating system designed specifically for Kubernetes, as a worker <code>Node</code> OS in Gardener <code>Shoot</code> clusters. This required moving away from the traditional <code>systemd</code>-based management model.</p><h3 id="motivation-benefits-4" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-4" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><ul><li><em>Security:</em> Talos offers enhanced security due to its immutable filesystem, minimal package set, and zero traditional access surfaces (no SSH or Shell).</li><li><em>Management:</em> The OS is fully declarative and configuration is entirely API-based (via GRPC), which aligns with Kubernetes principles.</li><li><em>Architecture:</em> It provides a lightweight solution dedicated solely to running Kubernetes, potentially leading to a smaller attack surface and improved stability.</li></ul><h3 id="achievements-4" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-4" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><ul><li>We successfully demonstrated that Talos OS is technically feasible as a Gardener worker <code>Node</code>.</li><li><em>Machine bootstrapping:</em> Automated the generation of the initial Talos configuration and successfully deployed Talos <code>Node</code>s as <code>Pod</code>s in a local provider PoC, pushing the configuration while the <code>Node</code> was in &quot;insecure&quot; mode.</li><li><em>Cluster joining:</em> Established trust by configuring the <code>kubelet</code> with the cluster CA, successfully using bootstrap tokens, and identifying compatible <code>kubelet</code> images.</li><li><em>CNI &amp; VPN:</em> Successfully configured the cluster by disabling the Talos-native CNI, enabling the <code>kubelet</code> server for VPN/tunneling, and resolving an SNI issue by disabling <code>KubePrism</code> and ensuring proper DNS resolution.</li><li><em>Control plane trust:</em> Successfully deployed the <code>trustd</code> daemon in the control plane and configured Istio routing to allow Talos <code>Node</code>s to reach it securely.</li></ul><h3 id="next-steps-4" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-4" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>The Talos OS integration will not be followed up to production at this time, but the architectural findings point to the necessary evolution for future support of declarative operating systems:</p><ul><li><code>OperatingSystemConfig</code> &amp; <code>gardener-node-agent</code> evolution: Refactor the existing <code>OperatingSystemConfig</code> and Gardener Node Agent (GNA) to abstract away the current dependency on <code>systemd</code>, requiring a new agent that leverages Talos&#39;s declarative API.</li><li><em>Extension compatibility:</em> Extensions that inject <code>systemd</code> units via OSC mutations must be rewritten to use containerized sidecars or <code>DaemonSet</code>s.</li><li><em>Service Hardening:</em> Define a standard strategy for securely exposing the Talos API daemon (<code>apid</code>) to operators for debugging and create a fully managed deployment for the <strong><code>trustd</code></strong> component within the control plane.</li></ul><h3 id="code-pull-requests-4" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-4" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>No specific public pull requests are available, as this was a feasibility evaluation track.</li></ul><h2 id="üì¶-gardener-api-types-as-standalone-go-module" tabindex="-1">üì¶ Gardener API Types As Standalone Go Module <a class="header-anchor" href="#üì¶-gardener-api-types-as-standalone-go-module" aria-label="Permalink to &quot;üì¶ Gardener API Types As Standalone Go Module&quot;">‚Äã</a></h2><h3 id="problem-statement-5" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-5" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>The <a href="https://github.com/gardener/gardener" target="_blank" rel="noreferrer">main Gardener repository</a> currently contains all API types within its primary Go module. This forces external projects that only need to import the API types (e.g., extensions, consumers) to download and depend on the entire set of dependencies for the main Gardener component, leading to unnecessary overhead and potential dependency conflicts.</p><h3 id="motivation-benefits-5" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-5" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>Introducing a dedicated Go module for <code>pkg/apis</code> (the API types) ensures a minimal set of dependencies for consumers who only need the API definitions. This significantly reduces the size and complexity of the dependency graph for API-only imports.</p><h3 id="achievements-5" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-5" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><p>We opened a pull request to implement the changes. ü•≥</p><h3 id="next-steps-5" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-5" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>The following tasks are necessary to fully implement the standalone API module:</p><ul><li>Introduce a dedicated Go module for <code>pkg/apis</code> in <code>gardener/gardener</code>.</li><li>Strictly limit the dependencies of this API module (e.g., only to <code>k8s.io/{api,apimachinery,utils}</code>).</li><li>Enforce dependency restrictions using a <code>.import-restrictions</code> file.</li><li>Ensure the API module is released together with the main module, using the proper Go submodule tag (similar to <a href="https://github.com/gardener/cc-utils/pull/1382" target="_blank" rel="noreferrer">gardener/cc-utils#1382</a>).</li><li>Use the Go workspaces feature in <code>gardener/gardener</code> to conveniently develop both the main and the API module together.</li></ul><h3 id="code-pull-requests-5" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-5" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>Fixes issue: <a href="https://github.com/gardener/gardener/issues/2871" target="_blank" rel="noreferrer">gardener/gardener#2871</a></li><li>PR: <a href="https://github.com/gardener/gardener/pull/13536" target="_blank" rel="noreferrer">gardener/gardener#13536</a></li></ul><h2 id="üìà-gardener-scale-out-tests" tabindex="-1">üìà Gardener Scale-Out Tests <a class="header-anchor" href="#üìà-gardener-scale-out-tests" aria-label="Permalink to &quot;üìà Gardener Scale-Out Tests&quot;">‚Äã</a></h2><h3 id="problem-statement-6" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-6" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>We don&#39;t have a good estimate of how many <code>Seed</code>s and <code>Shoot</code>s a Gardener environment can support. We are not aware of any scalability limitations that we might face in the future. There is no way to prevent regressions in Gardener&#39;s scalability.</p><h3 id="motivation-benefits-6" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-6" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>We wanted to implement &quot;hollow&quot; <code>gardenlet</code>s, similar to <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scalability/kubemark-guide.md" target="_blank" rel="noreferrer">Kubemark&#39;s</a> hollow <code>Node</code>s, and run many of them to generate load on the Garden cluster. This approach provides a solid foundation for running automated performance/scalability tests.</p><h3 id="achievements-6" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-6" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><p>We transferred the concept of hollow <code>Node</code>s to <code>Seed</code>s, meaning that we have a <code>gardenlet</code> that registers itself with the Garden and reports itself as ready. <code>Shoot</code>s scheduled to these <code>Seed</code>s are also reported healthy, but no real control planes will be spawned. To simulate a more realistic scenario, we analyzed the queries-per-second (QPS) that the <code>gardenlet</code>s perform against the Gardener API server (GAPI). We registered a runnable in the hollow <code>gardenlet</code> that simulates these requests, based on the number of <code>Shoot</code>s on the <code>Seed</code>s. In the local operator setup on an M4 Mac with 48 GB of memory, we were able to schedule ~200 hollow <code>gardenlet</code>s in the runtime cluster before we ran out of resources. We figured out that scheduling too many <code>Shoot</code>s and <code>Seed</code>s in a small amount of time leads to problems, specifically lease requests timing out. The settings of the Garden were just taken as-is, so tuning for this scenario would probably be possible. We did a longer running test, aiming for 50 <code>Shoot</code>s per minute and 1 <code>Seed</code> every 3 minutes, to circumvent the initial problem. The outcome was 800 <code>Seed</code>s and 21,600 <code>Shoot</code>s over 10 hours, but around 4 am, the test ran into a meltdown. We observed unequal request balancing on the Kubernetes API servers (KAPIs), as no Layer 7 (L7) load balancing was active.</p><h3 id="next-steps-6" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-6" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>We can try to run the tests in a more controlled environment. Although we will not continue with this after the hackathon, this artifact can serve as a starting point for the next round.</p><h3 id="code-pull-requests-6" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-6" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>Hollow <code>gardenlet</code> WIP implementation: <a href="https://github.com/acumino/gardener/tree/hollow-gardenlet" target="_blank" rel="noreferrer">acumino/gardener#hollow-gardenlet</a></li></ul><p><img src="'+n+'" alt="concept diagram for the gardener scale-out tests using hollow clusters and gardenlets to simulate load"></p><h2 id="‚ù§Ô∏è‚Äçü©π-force-restore-operation-annotation-for-shoots" tabindex="-1">‚ù§Ô∏è‚Äçü©π <code>force-restore</code> Operation Annotation For <code>Shoot</code>s <a class="header-anchor" href="#‚ù§Ô∏è‚Äçü©π-force-restore-operation-annotation-for-shoots" aria-label="Permalink to &quot;‚ù§Ô∏è‚Äçü©π `force-restore` Operation Annotation For `Shoot`s&quot;">‚Äã</a></h2><h3 id="problem-statement-7" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-7" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>The existing mechanism for disaster recovery of a shoot cluster using available backups needs improvement. We aimed to introduce a clear way for operators to force a restore operation using existing backups, which is particularly critical in disaster scenarios.</p><h3 id="motivation-benefits-7" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-7" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><ul><li>Facilitates and simplifies the recovery process for <code>Shoot</code> clusters from a disaster event.</li><li>Provides an annotation for operators to initiate a restore using the last known good state from the available backups.</li><li>This feature is general enough to be adopted by provider extensions like OpenStack to manage specific resource recovery flows.</li></ul><h3 id="achievements-7" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-7" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><ul><li>Identified the necessary changes in the <code>gardenlet</code>&#39;s operational logic, specifically around backup handling and migration checks.</li><li>The implementation requires updates to how the <code>gardenlet</code> determines if a copy of backups is required. It needs to handle cases where the source <code>BackupEntry</code> is not found, preventing errors during recovery attempts.</li></ul><h3 id="next-steps-7" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-7" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><ul><li>The identified <code>gardenlet</code> changes must be fully tested and implemented to enable the core <code>force-restore</code> functionality.</li><li>An issue in <a href="https://github.com/gardener/gardener-extension-provider-openstack" target="_blank" rel="noreferrer">gardener-extension-provider-openstack</a> that hinders control plane migration needs to be fixed.</li></ul><h3 id="code-pull-requests-7" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-7" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>Related issue for core Gardener functionality: <a href="https://github.com/gardener/gardener/issues/12952" target="_blank" rel="noreferrer">gardener/gardener#12952</a></li><li>Identified issue in the OpenStack provider extension: <a href="https://github.com/gardener/gardener-extension-provider-openstack/issues/1217" target="_blank" rel="noreferrer">gardener/gardener-extension-provider-openstack#1217</a></li></ul><h2 id="üóÉÔ∏è-go-build-cache-in-prow" tabindex="-1">üóÉÔ∏è Go Build Cache In Prow <a class="header-anchor" href="#üóÉÔ∏è-go-build-cache-in-prow" aria-label="Permalink to &quot;üóÉÔ∏è Go Build Cache In Prow&quot;">‚Äã</a></h2><p><a href="https://github.com/shafeeqes" target="_blank" rel="noreferrer">Shafeeque</a> and <a href="https://github.com/shegox" target="_blank" rel="noreferrer">Tobias</a> explored the build and test caching for <a href="https://prow.gardener.cloud/" target="_blank" rel="noreferrer">Gardener Prow</a> jobs. During this process, we discovered some interesting aspects of how Go caching works, its usage, and how the new Go feature <a href="https://pkg.go.dev/cmd/go/internal/cacheprog" target="_blank" rel="noreferrer"><code>GOCACHEPROG</code></a> operates.</p><p>We pursued three goals:</p><ol><li>Speed up time-to-feedback on pull requests.</li><li>Reduce load on the Prow build clusters.</li><li>Do this securely.</li></ol><p>For security, it was critical that presubmit (pull request) jobs could read from the cache but never write to it. This prevents untrusted PRs and potentially broken jobs from polluting the cache.</p><p>We first reviewed how other projects handle caching:</p><ul><li><strong>Istio:</strong> Uses a <a href="https://github.com/istio/test-infra/blob/c739e67b9aa67b0ffec6917a49f5e235e1fe0872/prow/cluster/jobs/istio/istio/istio.istio.master.gen.yaml#L38-L51" target="_blank" rel="noreferrer">hostPath volume</a> to reuse cache on the same node. This helps somewhat, but suffers from numerous cache misses as jobs are assigned to different nodes. With our autoscaling worker groups, node churn would make misses even more common, so this wouldn‚Äôt be effective for us.</li><li><strong>Kubermatic machine-controller:</strong> Uses scripts to <a href="https://github.com/kubermatic/machine-controller/blob/345eaa102974eda999b6cc59c9e21116b4e81e8e/hack/ci/download-gocache.sh" target="_blank" rel="noreferrer">fetch a cache archive</a> for the PR&#39;s ancestor commit from blob storage before the build, and <a href="https://github.com/kubermatic/machine-controller/blob/345eaa102974eda999b6cc59c9e21116b4e81e8e/hack/ci/upload-gocache.sh" target="_blank" rel="noreferrer">upload an updated cache</a> on main after the build. This mirrors systems like GitHub Actions‚Äô actions/cache and works with plain file systems, so build tooling doesn‚Äôt need remote cache awareness.</li></ul><p>We considered using a ReadWriteMany persistent volume for caching to achieve a similar setup to Istio, but across multiple nodes. On GCP, the practical option is NFS Filestore, which is quite expensive. Additionally, to make presubmits read-only, we‚Äôd have to copy the cache to a local directory before builds‚Äîan expensive step for large caches that erodes the performance benefits.</p><p>Go recently introduced the <code>GOCACHEPROG</code> feature, which allows you to plug in a custom program to read from and write to the build and test cache. We&#39;re using it with the open-source <a href="https://github.com/saracen/gobuildcache" target="_blank" rel="noreferrer"><code>saracen/gobuildcache</code></a> to store cache entries in Google Cloud Storage.</p><ul><li>This gives us a &quot;remote cache helper&quot; without needing to download/upload an entire cache directory before/after each build.</li><li>Because Go‚Äôs cache is highly granular (per compiled/tested unit), we only fetch what&#39;s needed rather than relying on commit ancestry heuristics.</li><li>Google Cloud Storage doesn&#39;t charge network costs within the same region and only minimal costs for storage and operations.</li><li>Trade-off: many small requests to the remote backend are slower than local filesystem access. However, with our high hit rate, the overall impact is very positive.</li></ul><p>We use separate GCP principals, federated from the Prow shoot cluster via Workload Identity Federation, with read-only permissions for presubmit jobs and read-write permissions for postsubmit and periodic jobs.</p><p>With this setup, we have already significantly accelerated builds. However, some parts, like <code>go test</code> and linting, didn‚Äôt speed up as much. We found that <code>go test</code> only caches results when a limited set of flags is used; in Prow, we rely on <code>ginkgo.junit-report=junit.xml</code> to produce JUnit reports for pull requests, and this flag disables test result caching. In local experiments, removing this flag enabled caching and dramatically reduced unit test time (for example, from approximately 40 minutes to under 5 minutes).</p><p>Overall, we‚Äôre happy with the first improvements. Examining Gardener‚Äôs jobs, we did not significantly reduce end-to-end feedback time for presubmits; however, the actual CPU time spent on builds decreased by more than 90%. The impact is particularly clear in the kind-e2e tests: they used to take roughly 1 hour 30 minutes and consume about 65 minutes of CPU time across 12 cores in parallel at the beginning; now they consume under 3 minutes of CPU time across fewer than 2 cores. This substantially reduces the load on the build cluster.</p><h3 id="kind-e2e-tests-without-cache" tabindex="-1">kind e2e tests without cache <a class="header-anchor" href="#kind-e2e-tests-without-cache" aria-label="Permalink to &quot;kind e2e tests without cache&quot;">‚Äã</a></h3><div class="language-terminaloutput vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">terminaloutput</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>real        85m20.914s</span></span>\n<span class="line"><span>user        64m51.059s</span></span>\n<span class="line"><span>sys          6m42.249s</span></span></code></pre></div><p><img src="'+s+`" alt="Prow CPU metrics without cache showing 12 CPUs being used during build"></p><h3 id="kind-e2e-tests-with-cache" tabindex="-1">kind e2e tests with cache <a class="header-anchor" href="#kind-e2e-tests-with-cache" aria-label="Permalink to &quot;kind e2e tests with cache&quot;">‚Äã</a></h3><div class="language-terminaloutput vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">terminaloutput</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>real        82m53.159s</span></span>
<span class="line"><span>user         2m33.118s</span></span>
<span class="line"><span>sys          1m0.021s</span></span></code></pre></div><p><img src="`+l+'" alt="Prow CPU metrics with cache showing 2 CPUs being used during build"></p><h2 id="üõ†Ô∏è-mcm-update-machines-updates-during-in-place-updates" tabindex="-1">üõ†Ô∏è MCM: Update Machines Updates During In-Place Updates <a class="header-anchor" href="#üõ†Ô∏è-mcm-update-machines-updates-during-in-place-updates" aria-label="Permalink to &quot;üõ†Ô∏è MCM: Update Machines Updates During In-Place Updates&quot;">‚Äã</a></h2><h3 id="problem-statement-8" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-8" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>The existing in-place node update mechanism in the <a href="https://github.com/gardener/machine-controller-manager" target="_blank" rel="noreferrer">gardener/machine-controller-manager</a> (MCM) does not support updating underlying infrastructure resources, such as the OS image. To update these resources, a full node recreation is required, which we want to avoid.</p><h3 id="motivation-benefits-8" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-8" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>We want to enable infrastructure-level updates without requiring a full node recreation, improving efficiency and reducing disruption. The key benefit of bare metal is achieving a clean OS state by memory booting the machine via the network and updating the image by rebooting the server.</p><h3 id="achievements-8" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-8" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><ul><li>Extended the MCM provider driver interface by introducing a new <code>UpdateMachine</code> method for in-place updates.</li><li>The signature of the new method is <code>UpdateMachine(context.Context, *UpdateMachineRequest) (*UpdateMachineResponse, error)</code>.</li><li>Completed the MCM update machine implementation on a feature branch.</li><li>Successfully tested the concept on a local provider environment.</li><li>Released forked container images to test the scenario in a real-world bare metal environment.</li><li>Implemented a &quot;hacky&quot; fix to address the challenge of getting the new <code>MachineImage</code> within the <code>UpdateMachine</code> call, as only the old image was initially available.</li><li>Mitigated an issue where the <a href="https://github.com/gardener/gardener-extension-os-gardenlinux" target="_blank" rel="noreferrer">Garden Linux OS extension</a> used an on-disk-only <code>gardenlinux-update</code> command by providing a fix to a forked extension.</li></ul><h3 id="next-steps-8" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-8" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>We need to revisit the idea of using in-place updates for memory-booted servers and re-evaluate the rolling update approach instead. The current hacky changes to MCM are not worth contributing upstream, as they might break the core in-place update contract; therefore, we will not pursue them further.</p><h3 id="code-pull-requests-8" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-8" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>MCM update machine implementation: <a href="https://github.com/afritzler/machine-controller-manager/tree/enh/machine-update" target="_blank" rel="noreferrer">afritzler/machine-controller-manager:enh/machine-update</a></li><li>Testing on <code>provider-local</code>: <a href="https://github.com/aniruddha2000/gardener/tree/ani/machine-update" target="_blank" rel="noreferrer">aniruddha2000/gardener:ani/machine-update</a></li><li>Hacky fix for <code>MachineImage</code>: <a href="https://github.com/afritzler/machine-controller-manager/commit/7005bd20bea234895bdf9a9fc98e5dc43ceca1e9" target="_blank" rel="noreferrer">afritzler/machine-controller-manager@7005bd2</a></li><li>Fix forked Garden Linux OS extension: <a href="https://github.com/gardener/gardener-extension-os-gardenlinux/commit/6903635885b2a58d4c876612cd91be134f2aa307" target="_blank" rel="noreferrer">gardener/gardener-extension-os-gardenlinux@6903635</a></li></ul><h2 id="üîî-gardenadm-flow-package-handle-siginfo-t" tabindex="-1">üîî <code>gardenadm</code>/Flow Package: Handle <code>SIGINFO</code> (<code>^T</code>) <a class="header-anchor" href="#üîî-gardenadm-flow-package-handle-siginfo-t" aria-label="Permalink to &quot;üîî `gardenadm`/Flow Package: Handle `SIGINFO` (`^T`)&quot;">‚Äã</a></h2><h3 id="problem-statement-9" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-9" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>The existing command-line tools that utilize the Gardener <code>flow</code> package for complex, multistep operations currently lack a standardized way to provide clear, real-time feedback to the user about which step is currently executing. This makes the tools appear static or slow, hindering developer productivity.</p><h3 id="motivation-benefits-9" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-9" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><ul><li>Improves the usability and developer experience of Gardener&#39;s command-line tools by providing visual confirmation of progress.</li><li>Increases developer productivity by making it easier to monitor long-running processes, quickly see which step failed, and estimate completion time.</li><li>Provides a standardized interface (<code>CommandLineProgressReporter</code>) that can be easily integrated into any tool using the <code>flow</code> package, ensuring consistency across the ecosystem.</li></ul><h3 id="achievements-9" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-9" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><ul><li>We successfully implemented the <code>CommandLineProgressReporter</code> utility.</li><li>This new reporter integrates with the <code>flow</code> package to print the currently executing step to standard output.</li><li>The feature was integrated into the main Gardener repository, making it available for use in all Gardener command-line tools.</li></ul><h3 id="next-steps-9" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-9" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><ul><li>Integrate the <code>CommandLineProgressReporter</code> into all relevant Gardener command-line tools that use the <code>flow</code> package to ensure a consistent user experience.</li></ul><h3 id="code-pull-requests-9" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-9" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>PR: <a href="https://github.com/gardener/gardener/pull/13565" target="_blank" rel="noreferrer">gardener/gardener#13565</a></li></ul><h2 id="‚öñÔ∏èÔ∏è-load-balancer-controller-for-provider-local" tabindex="-1">‚öñÔ∏èÔ∏è Load Balancer Controller For <code>provider-local</code> <a class="header-anchor" href="#‚öñÔ∏èÔ∏è-load-balancer-controller-for-provider-local" aria-label="Permalink to &quot;‚öñÔ∏èÔ∏è Load Balancer Controller For `provider-local`&quot;">‚Äã</a></h2><h3 id="problem-statement-10" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-10" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>Our goal was to build a functional, real load balancer controller for the <code>provider-local</code> extension, moving beyond simple <code>Service</code> controller implementations. This allows us to properly emulate external load-balancing behavior in local development and testing environments, such as <a href="https://kind.sigs.k8s.io/" target="_blank" rel="noreferrer">kind</a>.</p><h3 id="motivation-benefits-10" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-10" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>Implementing a dedicated load balancer controller is essential for accurately testing features that rely on external IP address allocation and port exposure. This will be crucial for making <code>ManagedSeed</code> tests succeed, as they require a reliable load balancer implementation in the test <code>Shoot</code> clusters.</p><h3 id="achievements-10" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-10" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><p>Significant progress has been made on implementation, as captured in the work-in-progress branch. The demo steps illustrate the core functionality achieved:</p><ul><li>IP addresses can now be allocated from a specified range (<code>172.18.255.64/26</code>) to <code>Services</code> of type <code>LoadBalancer</code>.</li><li>External access can be simulated by manually adding the allocated IP addresses to the host&#39;s loopback interface (<code>lo0</code>) and then accessing the exposed <code>Service</code> using those IPs.</li><li>The controller can utilize Docker networking features (<code>--enable-lb-port-mapping</code>) to map the load balancer port to the backing container.</li><li>This allows the containerized workload (like <a href="https://nginx.org/" target="_blank" rel="noreferrer">nginx</a>) to be reached externally via the allocated IP.</li><li>We demonstrated the successful creation and access of multiple <code>Service</code>s of type <code>LoadBalancer</code> (<code>nginx</code>, <code>nginx2</code>, etc.), each receiving a unique external IP from the defined pool.</li><li>A user can create a <code>Deployment</code>, expose it as a load balancer <code>Service</code>, and then curl the assigned external IP.</li></ul><h3 id="next-steps-10" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-10" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>The following tasks are planned to complete the load balancer controller:</p><ul><li>The controller needs to correctly handle IPv6.</li><li>We must implement the <code>LoadBalancer</code> controller for <code>Shoot</code>s to ensure <code>ManagedSeed</code> tests succeed.</li><li>Deployment of this functionality is required in the local setup.</li><li>The <code>provider-local</code> <code>Service</code> controller must be dropped, as its functionality will be superseded by the new, dedicated load balancer controller.</li></ul><h3 id="code-pull-requests-10" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-10" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>Branch: <a href="https://github.com/timebertt/cloud-provider-kind/tree/allocate-ips" target="_blank" rel="noreferrer">timebertt/cloud-provider-kind:allocate-ips</a></li></ul><h2 id="üîå-evaluation-of-nft-mode-in-kube-proxy" tabindex="-1">üîå Evaluation Of NFT Mode In <code>kube-proxy</code> <a class="header-anchor" href="#üîå-evaluation-of-nft-mode-in-kube-proxy" aria-label="Permalink to &quot;üîå Evaluation Of NFT Mode In `kube-proxy`&quot;">‚Äã</a></h2><h3 id="problem-statement-11" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-11" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>The existing <code>kube-proxy</code> modes, <code>ipvs</code> and <code>iptables</code>, have been the standard, but the Kubernetes community has stabilized the <code>nftables</code> mode in version <code>1.31</code>. We needed to assess this new mode for potential adoption within Gardener.</p><h3 id="motivation-benefits-11" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-11" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p><code>nftables</code> is recognized as the modern, more efficient, and flexible successor to the legacy <code>iptables</code> framework. Adopting this new proxy mode ensures we keep up with Kubernetes upstream developments and can leverage the performance improvements and simpler rule management that <code>nftables</code> offers.</p><h3 id="achievements-11" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-11" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><p>We successfully implemented initial support for the <code>nftables</code> proxy mode within Gardener. Cluster operators can now provision and test <code>Shoot</code> clusters using the new <code>nftables</code> mode for <code>kube-proxy</code>.</p><h3 id="next-steps-11" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-11" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>The next logical step is to perform comprehensive testing of the new mode under various loads and configurations. We will evaluate the possibility of setting <code>nftables</code> as the default <code>kube-proxy</code> mode for new <code>Shoot</code> clusters when they use a sufficiently recent Kubernetes version.</p><h3 id="code-pull-requests-11" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-11" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li><a href="https://github.com/gardener/gardener/pull/13558" target="_blank" rel="noreferrer">gardener/gardener#13558</a></li></ul><h2 id="üåâ-replace-ingress-nginx-controller-with-gateway-api" tabindex="-1">üåâ Replace Ingress NGINX controller With Gateway API <a class="header-anchor" href="#üåâ-replace-ingress-nginx-controller-with-gateway-api" aria-label="Permalink to &quot;üåâ Replace Ingress NGINX controller With Gateway API&quot;">‚Äã</a></h2><h3 id="problem-statement-12" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-12" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>Following the <a href="https://kubernetes.io/blog/2025/11/11/ingress-nginx-retirement/" target="_blank" rel="noreferrer">deprecation announcement for Ingress NGINX</a>, we need to evaluate and implement the Gateway API as a replacement for ingress traffic management in the Garden runtime/<code>Seed</code> clusters. This involves migrating from existing Istio-native resources (<code>Gateway</code>, <code>VirtualServices</code>, <code>DestinationRules</code>) to Gateway API resources, such as <code>HTTPRoute</code>, and overcoming initial integration hurdles with Gardener&#39;s Istio configuration.</p><h3 id="motivation-benefits-12" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-12" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><ul><li>The primary motivation is to replace the deprecated Ingress NGINX controller with a modern, standard API.</li><li>Adopting the Gateway API drives its maturity and aligns us with the future direction of Kubernetes <code>Ingress</code>.</li><li>This evaluation will determine how many Istio-native resources can be replaced, simplifying the configuration and management of ingress traffic over time.</li></ul><h3 id="achievements-12" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-12" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><ul><li>Successfully identified and fixed an issue preventing the Gateway API from working with Gardener&#39;s default Istio configuration due to restrictive <code>exportTo</code> settings in the mesh config. <ul><li>We discovered that implicitly allowing the internally created <code>VirtualService</code> by setting <code>defaultVirtualServiceExportTo: &#39;.&#39;</code> is required for the Istio ingress gateway to export services.</li></ul></li><li>Created a functional branch showcasing Gateway API usage for Plutono.</li><li>HTTP basic authentication was implemented using an <code>EnvoyFilter</code> and an external authorization server, with a special label introduced to reference basic auth secrets.</li><li>Extended the <code>gardener-resource-manager</code> network policy controller to generate network policies based on <code>HTTPRoute</code> resources, similar to how it handled <code>Ingress</code> resources.</li><li>Managed to serve Gateway API resources and Istio resources under the same port.</li><li>Translated the basic redirect logic (used for preventing access to admin routes) by using a redirect to &quot;/&quot; as a workaround for the current Gateway API limitation on direct responses.</li></ul><h3 id="next-steps-12" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-12" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><ul><li>Fully implement HTTP basic authentication, potentially replacing the custom <code>EnvoyFilter</code> with Istio-native external authorization or adopting the experimental <code>HTTPExternalAuthFilter</code> when Istio supports it.</li><li>Complete the translation of all remaining NGINX annotations and an existing <code>Ingress</code> resource to equivalent Gateway API resources.</li><li>The migration of <code>DestinationRules</code> to Gateway API&#39;s new traffic policy resources (like <code>XBackendTrafficPolicy</code>) needs to be finalized.</li><li>Implement traffic encryption for the external authorization server.</li><li>The deployment and configuration for the dashboard and Prometheus services (including Alertmanager) need to be migrated to the Gateway API.</li><li>The changes must be integrated into the operator reconciliation flow and the <code>Shoot</code> reconciliation flow.</li></ul><h3 id="code-pull-requests-12" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-12" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>WIP branch: <a href="https://github.com/metal-stack/gardener/tree/gateway-api" target="_blank" rel="noreferrer">metal-stack/gardener:gateway-api</a></li></ul><h2 id="üê±-add-support-for-calico-whisker" tabindex="-1">üê± Add Support For Calico Whisker <a class="header-anchor" href="#üê±-add-support-for-calico-whisker" aria-label="Permalink to &quot;üê± Add Support For Calico Whisker&quot;">‚Äã</a></h2><h3 id="problem-statement-13" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-13" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>We needed to integrate <a href="https://www.tigera.io/blog/calico-whisker-your-new-ally-in-network-observability/" target="_blank" rel="noreferrer">Calico Whisker</a> into Gardener-managed clusters to provide traffic monitoring and tracing capabilities, similar to <a href="https://github.com/cilium/hubble" target="_blank" rel="noreferrer">Cilium&#39;s Hubble</a>. Whisker is typically deployed via the <a href="https://github.com/tigera/operator" target="_blank" rel="noreferrer">tigera/operator</a>, which complicates its deployment in our existing Calico extension setup.</p><h3 id="motivation-benefits-13" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-13" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>Adding Calico Whisker provides valuable observability tools for cluster network traffic, which is a feature highly requested by users who prefer Calico as their Container Network Interface (CNI). This new capability brings Calico clusters closer to feature parity with the network tracing offered by Cilium/Hubble.</p><h3 id="achievements-13" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-13" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><ul><li>Developed a working prototype by reusing code from the <code>tigera-operator</code> to manage the deployment of the required components, which are Calico Whisker and Calico Goldmane.</li><li>The prototype successfully manages the mTLS communication requirement between <code>calico-node</code> and <code>calico-typha</code> by utilizing the Gardener <code>security-manager</code> for certificate provisioning.</li><li>Identified and implemented the necessary network policies to allow communication between the new components, such as the extension to <code>Shoot</code> API servers and <code>calico-node</code> to <code>goldmane</code>.</li><li>The code for the working prototype is available in a feature branch for further development.</li></ul><h3 id="next-steps-13" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-13" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><ul><li>The next primary step is the productization of the changes into the <a href="https://github.com/gardener/gardener-extension-networking-calico" target="_blank" rel="noreferrer">main Calico networking extension</a>, including integrating certificate management and component deployment logic.</li><li>We need to resolve the issue with the untagged <code>tigera-operator</code> API module dependency to ensure stable integration.</li><li>The approach used here, which involves reusing <code>tigera-operator</code> code, can potentially be applied to simplify the future addition of support for the Calico API server.</li></ul><h3 id="code-pull-requests-13" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-13" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>WIP branch: <a href="https://github.com/ScheererJ/gardener-extension-networking-calico/tree/calico/whisker" target="_blank" rel="noreferrer">ScheererJ/gardener-extension-networking-calico:calico/whisker</a></li></ul><h2 id="üè∑Ô∏è-respect-terminating-nodes-in-load-balancing" tabindex="-1">üè∑Ô∏è Respect Terminating <code>Node</code>s In Load-Balancing <a class="header-anchor" href="#üè∑Ô∏è-respect-terminating-nodes-in-load-balancing" aria-label="Permalink to &quot;üè∑Ô∏è Respect Terminating `Node`s In Load-Balancing&quot;">‚Äã</a></h2><h3 id="problem-statement-14" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-14" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>When a machine is deleted, the load balancer may not be immediately aware that the corresponding node is no longer available. This lack of synchronization can lead to unsuccessful new connections because the load balancer may still forward traffic to the terminating <code>Node</code>. The existing <code>Node</code> conditions or &quot;drain&quot; signals set by the Machine Controller Manager (MCM) do not reliably trigger a reconciliation in the <code>cloud-provider</code> package or cause the <code>kube-proxy</code> health check to fail.</p><h3 id="motivation-benefits-14" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-14" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>We want to improve load-balancing behavior during machine terminations to prevent connection failures. The goal is to provide a reliable signal to both the cloud load balancer (via <code>cloud-provider</code>) and the in-cluster service routing (via <code>kube-proxy</code>) that a <code>Node</code> is being terminated and should be removed from the rotation immediately.</p><h3 id="achievements-14" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-14" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><ul><li>Identified that the <code>ToBeDeletedByClusterAutoscaler</code> taint is recognized and handled by both the Kubernetes <code>cloud-provider</code> and <code>kube-proxy</code> components for draining traffic.</li><li>Implemented changes in MCM to apply the <code>ToBeDeletedByClusterAutoscaler</code> taint to a node when its corresponding <code>Machine</code> is marked for deletion.</li><li>Applying this taint triggers the necessary load balancer reconcile and service health check updates, ensuring traffic is stopped before the <code>Node</code> is fully gone.</li></ul><h3 id="next-steps-14" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-14" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>The pull request needs to be merged and released.</p><h3 id="code-pull-requests-14" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-14" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>Pull request: <a href="https://github.com/gardener/machine-controller-manager/pull/1054" target="_blank" rel="noreferrer">gardener/machine-controller-manager#1054</a></li><li>Reference to <code>cloud-provider</code> logic: <a href="https://github.com/kubernetes/cloud-provider/blob/080e91c4b910bf92dfd43ca79eb74f5d39dcba75/controllers/service/controller.go#L1027" target="_blank" rel="noreferrer">kubernetes/cloud-provider:controllers/service/controller.go#L1027</a></li><li>Reference to <code>kube-proxy</code> logic: <a href="https://github.com/kubernetes/kubernetes/blob/5bcb7599736327cd8c6d23e398002354a6e40f68/pkg/proxy/healthcheck/proxy_health.go#L187" target="_blank" rel="noreferrer">kubernetes/kubernetes:pkg/proxy/healthcheck/proxy_health.go#L187</a></li></ul><h2 id="üß∞-use-go-tools-drop-vgopath" tabindex="-1">üß∞ Use Go Tools &amp; Drop <code>VGOPATH</code> <a class="header-anchor" href="#üß∞-use-go-tools-drop-vgopath" aria-label="Permalink to &quot;üß∞ Use Go Tools &amp; Drop `VGOPATH`&quot;">‚Äã</a></h2><h3 id="problem-statement-15" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-15" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>The <a href="https://github.com/gardener/gardener" target="_blank" rel="noreferrer">main Gardener repository</a> relied on the outdated practice of using the <code>VGOPATH</code> environment variable within its build and development environment. This approach is generally obsolete in modern Go development, as it utilizes the Go modules system for dependency management and tooling.</p><h3 id="motivation-benefits-15" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-15" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>By migrating to standard Go tooling and removing <code>VGOPATH</code>, we achieve several benefits:</p><ul><li><em>Modernization:</em> We align our development and build process with current Go best practices (Go modules).</li><li><em>Simplification:</em> We removed the need for custom logic and non-standard environment variables (<code>VGOPATH</code>) in our tooling scripts, simplifying the development experience for contributors.</li><li><em>Dependency clarity:</em> We improved the clarity and reliability of our dependency resolution.</li></ul><h3 id="achievements-15" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-15" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><p>This effort was split into two key pull requests that together eliminate <code>VGOPATH</code> and standardize the use of Go tools:</p><ul><li>Remove <code>VGOPATH</code>: <ul><li>This PR introduced the initial change to remove all occurrences of <code>VGOPATH</code> from the <code>gardener/gardener</code> repository.</li><li>It refactored scripts and configurations that previously relied on this variable to use standard, module-aware Go commands.</li></ul></li><li>Use Go tools: <ul><li>This PR focused on simplifying our dependency setup. We updated our tool dependency management by pinning all necessary tools in the <code>go.mod</code> file using the standard <code>tools</code> directory pattern.</li><li>This means we no longer need complex, custom logic to ensure the correct versions of various Go tools (like linters, code generators, etc.) are available during development.</li><li>Tools are now sourced directly through <em>Go modules</em>, simplifying the bootstrapping process for a new developer.</li></ul></li></ul><h3 id="next-steps-15" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-15" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>The primary goal of removing <code>VGOPATH</code> and leveraging standard Go tools is now complete with the merging of these two PRs. The changes are integrated into the repository, ensuring a modernized and simplified development workflow for all contributors.</p><h3 id="code-pull-requests-15" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-15" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>Use go tools instead of <code>tools.go</code>: <a href="https://github.com/gardener/gardener/pull/13545" target="_blank" rel="noreferrer">gardener/gardener#13545</a></li><li>Remove <code>VGOPATH</code> and <code>hack/vgopath-setup.sh</code>: <a href="https://github.com/gardener/gardener/pull/13556" target="_blank" rel="noreferrer">gardener/gardener#13556</a></li></ul><h2 id="üîÄ-pod-overlay-to-native-routing-without-downtime" tabindex="-1">üîÄ Pod Overlay To Native Routing Without Downtime <a class="header-anchor" href="#üîÄ-pod-overlay-to-native-routing-without-downtime" aria-label="Permalink to &quot;üîÄ Pod Overlay To Native Routing Without Downtime&quot;">‚Äã</a></h2><h3 id="problem-statement-16" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-16" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>The current method for switching a <code>Shoot</code> cluster&#39;s networking mode between pod overlay networking and native routing requires a full reconfiguration and rollout, which results in network downtime. Many production clusters cannot tolerate this interruption, creating a need for a seamless, zero-downtime transition.</p><h3 id="motivation-benefits-16" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-16" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>Enabling seamless migration allows cluster operators to safely evolve networking configurations in critical production environments without service interruption. This improves the operational flexibility and resilience of Gardener-managed clusters. The desired state is that old nodes continue with the old mode while new <code>Node</code>s adopt the new mode, maintaining full cross-communication between all <code>Node</code>s during the transition.</p><h3 id="achievements-16" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-16" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><ul><li>Performed initial testing of the migration flow (bidirectional: overlay to native and native to overlay) in a small two-<code>Node</code> cluster using both Calico and Cilium CNIs on AWS.</li><li>Calico successfully maintained a long-running connection (netcat) and continuous traffic during the switch in both directions.</li><li>Cilium successfully transitioned from overlay to native routing without resetting the connection, though the reverse transition (native to overlay) consistently resulted in a connection reset.</li></ul><h3 id="next-steps-16" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-16" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><ul><li>This topic requires further investigation and will be addressed by <a href="https://github.com/docktofuture" target="_blank" rel="noreferrer">Sebastian</a> during ordinary operations as a direct follow-up to the related pull request (<a href="https://github.com/gardener/gardener/pull/13332" target="_blank" rel="noreferrer">gardener/gardener#13332</a>).</li><li>The Cilium connection reset issue during the native-to-overlay switch needs to be investigated and resolved to ensure robust zero-downtime migration for all supported CNIs.</li></ul><h3 id="code-pull-requests-16" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-16" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>The topic is a follow-up to ongoing work: <a href="https://github.com/gardener/gardener/pull/13332" target="_blank" rel="noreferrer">gardener/gardener#13332</a></li></ul><h2 id="üö™-gep-28-expose-api-server-of-self-hosted-shoots" tabindex="-1">üö™ [GEP-28] Expose API server Of Self-Hosted Shoots <a class="header-anchor" href="#üö™-gep-28-expose-api-server-of-self-hosted-shoots" aria-label="Permalink to &quot;üö™ [GEP-28] Expose API server Of Self-Hosted Shoots&quot;">‚Äã</a></h2><h3 id="problem-statement-17" tabindex="-1">Problem Statement <a class="header-anchor" href="#problem-statement-17" aria-label="Permalink to &quot;Problem Statement&quot;">‚Äã</a></h3><p>The API server of a <a href="https://github.com/gardener/gardener/blob/master/docs/proposals/28-self-hosted-shoot-clusters.md" target="_blank" rel="noreferrer">self-hosted <code>Shoot</code> cluster</a> with managed infrastructure needs a mechanism to be reliably exposed for external access. Currently, there is no standardized way for Gardener components to trigger cloud-specific load-balancing resources for this specific scenario.</p><h3 id="motivation-benefits-17" tabindex="-1">Motivation &amp; Benefits <a class="header-anchor" href="#motivation-benefits-17" aria-label="Permalink to &quot;Motivation &amp; Benefits&quot;">‚Äã</a></h3><p>We want to enable external access to the API server of self-hosted <code>Shoot</code>s by reusing existing cloud provider mechanisms, such as the cloud-controller-manager, creating a load balancer <code>Service</code> for <code>default/kubernetes</code>. The ability to expose the API server this way allows the <code>DNSRecord</code> to be updated with the LoadBalancer&#39;s external IP, providing robust, highly available external access.</p><h3 id="achievements-17" tabindex="-1">Achievements <a class="header-anchor" href="#achievements-17" aria-label="Permalink to &quot;Achievements&quot;">‚Äã</a></h3><ul><li>Introduced a new resource, <code>SelfHostedShootExposure</code>, in the <code>extensions.gardener.cloud/v1alpha1</code> API to manage the exposure resources. This resource abstracts the specific cloud exposure implementation (e.g., load balancer or <code>kube-vip</code>) into an extension model.</li><li>The <code>Shoot</code> API was extended to allow configuration of control plane exposure via <code>.spec.provider.workers[].controlPlane.exposure</code>.</li><li>Defined the responsibilities: <code>gardenadm</code> or <code>gardenlet</code> will create the <code>SelfHostedShootExposure</code> object, and a corresponding extension controller will reconcile the necessary cloud resources.</li><li>An <code>Actuator</code> interface was defined for the extension controllers with <code>Reconcile</code> and <code>Delete</code> methods to manage the exposure resources and report the ingress status (<code>[]corev1.LoadBalancerIngress</code>).</li><li>The <code>gardenlet</code> will also run a new controller to watch control plane <code>Node</code>s and automatically update the <code>SelfHostedShootExposure</code> resource&#39;s <code>.spec.endpoints[]</code> with the latest <code>Node</code> addresses.</li></ul><h3 id="next-steps-17" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps-17" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h3><p>Finish writing the <a href="https://github.com/gardener/gardener/blob/master/docs/proposals/README.md" target="_blank" rel="noreferrer">Gardener Enhancement Proposal (GEP)</a> and defend it in front of the <a href="https://gardener.cloud/community/steering/" target="_blank" rel="noreferrer">Technical Steering Committee (TSC)</a> for approval.</p><h3 id="code-pull-requests-17" tabindex="-1">Code &amp; Pull Requests <a class="header-anchor" href="#code-pull-requests-17" aria-label="Permalink to &quot;Code &amp; Pull Requests&quot;">‚Äã</a></h3><ul><li>Issue: <a href="https://github.com/gardener/gardener/issues/2906" target="_blank" rel="noreferrer">gardener/gardener#2906</a></li></ul><h2 id="ü§ñ-tool-enabled-agent-for-shoots" tabindex="-1">ü§ñ Tool-Enabled Agent For Shoots <a class="header-anchor" href="#ü§ñ-tool-enabled-agent-for-shoots" aria-label="Permalink to &quot;ü§ñ Tool-Enabled Agent For Shoots&quot;">‚Äã</a></h2><p>We developed agents to support operators with end-user questions about Gardener or a specific <code>Shoot</code> cluster. They use LLMs to plan actions and use powerful tools. The primary tools include knowledge-base search, ticket-database search, access to <code>Garden</code>/<code>Seed</code>/<code>Shoot</code> clusters via <code>bash</code>/<code>kubectl</code>, and the ability to extract metrics, logs, and more. This allows the agent to drill deep and surface insights that would otherwise require an operator a significant amount of time. Of course, LLMs won&#39;t always draw correct conclusions, but even then, the agent gives the operator a head start from which they can continue the investigation.</p><p><strong>Conclusion:</strong> The tooling for the platform has been successfully implemented and is currently in use. However, it is not yet publicly available due to the need to scrub internal data from its knowledge sources.</p><hr><p><img src="https://apeirora.eu/assets/img/BMWK-EU.png" alt="ApeiroRA"></p>',234)]))}const v=t(d,[["render",c]]);export{f as __pageData,v as default};
